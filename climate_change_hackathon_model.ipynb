{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNYvU0tPPneTZ97Mu22TojZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/livinNector/climate-change-hackathon/blob/main/climate_change_hackathon_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://oauth2: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@github.com/Ananthzeke/climate-change-hackathon.git"
      ],
      "metadata": {
        "id": "0tHwIjLUtsY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d rohanrao/air-quality-data-in-india"
      ],
      "metadata": {
        "id": "vDasvAFHdQhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cond-rnn"
      ],
      "metadata": {
        "id": "1pVxiDDDvdEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2sV78zmvJkR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as tfl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from cond_rnn import ConditionalRecurrent"
      ],
      "metadata": {
        "id": "zG3F0TZlwMhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "E8S2jSqN9Dvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.gstatic.com/covid19/mobility/Region_Mobility_Report_CSVs.zip\n",
        "!unzip Region_Mobility_Report_CSVs.zip -d region_mobility_report"
      ],
      "metadata": {
        "id": "DqJ9GEuCi1Iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"region_mobility_report/2020_IN_Region_Mobility_Report.csv\")\n",
        "df = pd.read_csv(\"region_m\")\n",
        "df"
      ],
      "metadata": {
        "id": "MNuvSUnKjNqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "telangana_df = df[df[\"sub_region_1\"]==\"Telangana\"].drop(columns=[\"country_region_code\",\"country_region\",\"sub_region_1\",\"iso_3166_2_code\",\"census_fips_code\",\"metro_area\",\"place_id\"])\n",
        "telangana_df"
      ],
      "metadata": {
        "id": "s5YQuNs1kgzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "locations = [\"Adilabad\",\"Nizamabad\",\"Warangal\",\"Karimnagar\",\"Khammam\"]"
      ],
      "metadata": {
        "id": "csqHEX61nHkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "telangana_df[telangana_df[\"sub_region_2\"].isin(locations)]"
      ],
      "metadata": {
        "id": "A1eN-b2c3R8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing aaq-aqi dataset"
      ],
      "metadata": {
        "id": "RGYNvV_QKliU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/climate-change-hackathon/Dataset/updated_aqi_and_aaq.csv\",parse_dates=True)\n",
        "df[\"Date\"] = df[\"Date\"].apply(lambda x: datetime.strptime(x,\"%Y-%m-%d\") )\n",
        "df[\"year\"] = df[\"Date\"].apply(lambda x: x.year-2016)\n",
        "df[\"month_sin\"] = df[\"Date\"].apply(lambda x: np.sin((x.month-1)/12*2*np.pi))\n",
        "df[\"month_cos\"] = df[\"Date\"].apply(lambda x: np.cos((x.month-1)/12*2*np.pi))\n",
        "df.drop(columns = [\"Toluene\",\"Xylene\",\"Benzene\",\"Date\"],inplace=True)\n",
        "df.fillna(0,inplace=True)\n",
        "df.columns = [\"location\",\"lat\",\"long\",\"co\",\"pm2.5\",\"nh3\",\"pm10\",\"nox\",\"o3\",\"so2\",\"aqi\",\"alt\",\"year\",\"month_sin\",\"month_cos\"]"
      ],
      "metadata": {
        "id": "DX2TGv8P8XL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aqi_features = [\"aqi\",\"so2\",\"nox\",\"pm10\",\"pm2.5\",\"co\",\"o3\",\"nh3\"]\n",
        "meteorological_features = [\"rainfall\",\"humid_min\",\"humid_max\",\"temp_min\",\"temp_max\",\"wind_speed\"]\n",
        "temporal_features = [\"month_sin\",\"month_cos\",\"year\"]\n",
        "time_series_features = aqi_features+meteorological_features+temporal_features\n",
        "geo_spatial_features = [\"lat\",\"long\",\"alt\"]\n",
        "all_features = time_series_features+geo_spatial_features\n",
        "\n",
        "out_feature_names = [\"aqi_out\",\"temp_max_out\",\"humid_max_out\"]"
      ],
      "metadata": {
        "id": "s03QOq2-w4J0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for feature in all_features:\n",
        "  if feature not in df.columns:\n",
        "    df[feature] = 0\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "hqAjA2hbQe2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aqi_locations = df[[\"location\",\"lat\",\"long\"]]\n",
        "# aqi_locations[\"location\"] = aqi_locations[\"location\"].apply(str.strip)\n",
        "aqi_locations = aqi_locations.groupby([\"lat\",\"long\"]).agg(lambda x: \"-\".join(x))\n",
        "len(aqi_locations)"
      ],
      "metadata": {
        "id": "IKe-06ik4oy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from geopy.geocoders import GeoNames\n",
        "\n",
        "geolocator = GeoNames(username=\"livinnector2001\",user_agent=\"hai\")\n",
        "locations_geocode = {loc:geolocator.geocode(loc) for loc in locations}\n",
        "\n"
      ],
      "metadata": {
        "id": "prsbJ419pdR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "location_dfs = [df[df[\"location\"]== location].drop(columns=[\"location\"])  for location in df[\"location\"].unique()]"
      ],
      "metadata": {
        "id": "c0ZpNWy2_VFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "location_datasets = [tf.data.Dataset.from_tensor_slices(dict(l_df)) for l_df in location_dfs]"
      ],
      "metadata": {
        "id": "H8CNYs0a-qOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dataset(ds):\n",
        "  x = {k:v[:24] for k,v in ds.items()}\n",
        "  \n",
        "  # geospatial features are same through out the window thus take only the first one\n",
        "  x[\"lat\"] = x[\"lat\"][0:1]\n",
        "  x[\"long\"] = x[\"long\"][0:1]\n",
        "  x[\"alt\"] = x[\"alt\"][0:1]\n",
        "  \n",
        "  for feature in geo_spatial_features:\n",
        "    x[feature].set_shape([1])\n",
        "    \n",
        "  for feature in time_series_features:\n",
        "    x[feature].set_shape([24])\n",
        "    x[feature]= tf.expand_dims(x[feature],axis=-1)\n",
        "\n",
        "  y = {}\n",
        "  y[\"aqi_out\"] = ds[\"aqi\"][24:]\n",
        "  y[\"temp_max_out\"] = ds[\"temp_max\"][24:]\n",
        "  y[\"humid_max_out\"] = ds[\"humid_max\"][24:]\n",
        "\n",
        "  for feature in out_feature_names:\n",
        "    y[feature].set_shape([12])\n",
        "    y[feature]= tf.expand_dims(y[feature],axis=-1)\n",
        "    \n",
        "    \n",
        "  return x,y\n",
        "\n",
        "\n",
        "location_windowed = [\n",
        "    l_ds\\\n",
        "    .window(36,shift=1,drop_remainder=True)\\\n",
        "    .flat_map(lambda x: tf.data.Dataset.zip({k:v.batch(36) for (k, v) in x.items()}))\\\n",
        "    .map(process_dataset)\n",
        "\n",
        "    for l_ds in location_datasets\n",
        "]\n",
        "location_all_windowed = tf.data.Dataset.from_tensor_slices(location_windowed).flat_map(lambda x:x).batch(32).cache()"
      ],
      "metadata": {
        "id": "Rb7uWXYzAL3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "location_all_windowed"
      ],
      "metadata": {
        "id": "RcvE7UOZkxy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "N7-o8QryMh7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneToManyRNN(tf.keras.layers.Layer):\n",
        "  def __init__(self,rnn_cell,n_outputs,**kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.cell = rnn_cell\n",
        "    self.n_outputs = n_outputs\n",
        "\n",
        "  def call(self,input,state):\n",
        "    prediction = input\n",
        "    predictions = []\n",
        "    for i in range(self.n_outputs):\n",
        "      prediction,state = self.cell(prediction,state)\n",
        "      predictions.append(prediction)\n",
        "    \n",
        "    return tf.transpose(tf.stack(predictions),[1,0,2])"
      ],
      "metadata": {
        "id": "RrvlVQjgrE8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_normalization_layer(feature,ds):\n",
        "  norm = tfl.Normalization()\n",
        "  norm.adapt(ds.map(lambda x,y:x[feature]))\n",
        "  return norm"
      ],
      "metadata": {
        "id": "b_Jvw3XI8h3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [tf.keras.Input(shape=(24,1),name=name) for name in aqi_features+temporal_features]\n",
        "input_norms = [get_normalization_layer(feature,location_all_windowed)(input) for feature,input in zip(aqi_features+temporal_features,inputs)]\n",
        "inputs_concat = tf.keras.layers.concatenate(input_norms,name=\"time_series_inputs\")\n",
        "\n",
        "input_dense = tfl.Dense(8,activation=\"relu\",kernel_regularizer=\"l1\")(inputs_concat)\n",
        "input_norm = tfl.BatchNormalization()(input_dense)\n",
        "\n",
        "cond_inputs = [tf.keras.Input(shape=(1),name=name) for name in geo_spatial_features]\n",
        "cond_norms = [get_normalization_layer(feature,location_all_windowed)(input) for feature,input in zip(geo_spatial_features,cond_inputs)]\n",
        "cond_concat = tf.keras.layers.concatenate(cond_norms,name=\"conditional_inputs\")\n",
        "cond_concat = tfl.Dense(2,activation=\"relu\",kernel_regularizer=\"l1\")(cond_concat)\n",
        "cond_norm = tfl.BatchNormalization()(cond_concat)\n",
        "\n",
        "encoder_output,*encoder_state = ConditionalRecurrent(tfl.LSTM(8,activation=\"relu\",return_state=True),name=\"conditional_encoder\")([input_norm,cond_norm])\n",
        "\n",
        "x  = OneToManyRNN(tfl.LSTMCell(8,activation=\"relu\"),12,name=\"decoder\")(encoder_output,encoder_state)\n",
        "# x = tfl.LSTM(8,activation=\"relu\",return_sequences=True)(x)\n",
        "\n",
        "aqi_out = tfl.Dense(1,activation = \"relu\",name=\"aqi_out\")(x)\n",
        "# temp_high_out = tfl.Dense(1,activation = \"relu\",name=\"temp_max_out\")(x)\n",
        "# humid_high_out = tfl.Dense(1,activation = \"relu\",name=\"humid_max_out\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs = inputs+cond_inputs,outputs = [aqi_out])\n",
        "# model = tf.keras.Model(inputs = inputs+cond_inputs,outputs = [aqi_out,temp_high_out,humid_high_out])\n"
      ],
      "metadata": {
        "id": "PYFgsqflFhEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(model,rankdir=\"LR\",show_shapes=True)"
      ],
      "metadata": {
        "id": "3TICjE0n_wSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "9jE1EdLKpkyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.set_epsilon(1)"
      ],
      "metadata": {
        "id": "8mYUCw3WE_vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\",loss=\"mse\",metrics=[\"mae\",\"mape\"],run_eagerly=True)"
      ],
      "metadata": {
        "id": "ey810heWtrAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(location_all_windowed.map(lambda x,y:(x,y[\"aqi_out\"])),epochs=20)"
      ],
      "metadata": {
        "id": "4Bkk9T39iyWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "data{\n",
        "  lat : val,\n",
        "  long : val,\n",
        "  features: {\n",
        "    f1:[vals],\n",
        "    f2:[vals],\n",
        "    f3:[vals],\n",
        "  }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "2ZDg40jYxEtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input_feature(name):\n",
        "  \n"
      ],
      "metadata": {
        "id": "NVirpQ_owSWZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}